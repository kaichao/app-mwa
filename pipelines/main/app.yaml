name: main.app-mwa
label: mwa-comput
cluster: ${CLUSTER}
parameters:
  initial_status: RUNNING
  message_router: message-router-main
  default_sleep_count: 200

jobs:
  dir-list:
    base_image: hub.cstcloud.cn/scalebox/dir-listx
    variables:
      slot_options: slot_on_head
    environments:
      - REGEX_FILTER=${REGEX_FILTER}

  cluster-dist:
    base_image: ${FILE_COPY}
    variables:
      code_path: ${CODE_BASE}/dockerfiles/cluster-dist/code
      task_timeout_seconds: 1800
      # 10TB读缓存的临时空间
      dir_limit_gb: ${SHARED_ROOT}/tar~12800
    parameters:
      # 1257010784/1257010786_1257010815_ch109.dat.tar.zst
      # 以时间排序的打包文件
      key_group_regex: ^([0-9]+)/([0-9]+_[0-9]+)_ch([0-9]+).+$
      key_group_index: 1,2,3
    environments:
      - TARGET_URL=${SHARED_ROOT}/tar
    hosts:
      - n0:2

  pull-unpack:
    base_image: ${FILE_COPY}
    # node1上的文件读取错误，需设置特权权限。可能更新该节点上docker到最新版本可解决该问题
    command: ${PRIVILEGED_COMMAND}
    variables:
      code_path: ${CODE_BASE}/dockerfiles/pull-unpack/code
      task_timeout_seconds: 1800
      # 2 tar.zst files allowed in tmpfs
      # 下采样基本完成后，再重新传远端的打包文件（tar.zst）
      # SSD最多存放2组数据，beam-maker单次处理150秒，则最多存放300秒数据（300秒 * 313MB/秒 = 92GB ）
      # dir_limit_gb: ${UNPACK_DIR_LIMIT_GB}
      dir_free_gb: ${UNPACK_DIR_FREE_GB}
      # 4 * 30s = 120s
      progress_counter_diff: 120
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      # 1257010784/1257010786_1257010815_ch114.dat.tar.zst
      key_group_regex: ^([0-9]+)/([0-9]+_[0-9]+)_ch([0-9]+).+$
      key_group_index: 1,2,3
      # message_router_index: 1
      retry_rules: "['2:3']"
    environments:
      - LOCAL_OUTPUT_ROOT=${LOCAL_DISK_ROOT}
      - JUMP_SERVERS=${JUMP_SERVERS}
      - WITH_HEADERS=yes
      # 可给message-router发送重复名的消息（覆盖）
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      -  ${NODES}:1

  beam-maker:
    label: beam-maker
    base_image: ${MWA_VCSTOOLS}
    command: ${ROCM_COMMAND}
    variables:
      task_timeout_seconds: ${BEAM_MAKER_TIMEOUT}
      dir_free_gb: ${BEAM_MAKER_DIR_FREE_GB}
      code_path: ${CODE_BASE}/dockerfiles/mwa-vcstools/beam-maker/code
      # output_text_size: 1048576
      progress_counter_diff: 96
      slot_options: tmpfs_workdir
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      slot_timeout_seconds: 60
      # 1257010784/1257010786_1257010845/109/00001_00024
      key_group_regex: ^([0-9]+)/([0-9]+_[0-9]+)/([0-9]{3})/([0-9]{5}_[0-9]{5})$
      # 若节点少，每个节点处理多于一个channel，则为 1,3,2
      # 若节点多，每个节点处理1个channel，则顺序为：1,2,4
      key_group_index: 1,3,2
      # beam-maker的message-router耗时长，独立处理
      message_router_index: 1
    environments:
      - LOCAL_INPUT_ROOT=${LOCAL_DISK_ROOT}
      # - LOCAL_CAL_ROOT=${LOCAL_SHM_ROOT}
      - LOCAL_OUTPUT_ROOT=${LOCAL_SHM_ROOT}
      - ZSTD_TARGET_FILE=no
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
      # 通常无KEEP_SOURCE_FILE设置，由信号量统一管理何时删除原始文件，仅压力测试时需使用
    hosts:
      - ${NODES}:4

  down-sampler:
    base_image: ${DOWN_SAMPLER}
    command: ${PRIVILEGED_COMMAND}
    variables:
      task_timeout_seconds: ${DOWN_SAMPLER_TIMEOUT}
      dir_free_gb: ${DOWN_SAMPLER_DIR_FREE_GB}
      code_path: ${CODE_BASE}/dockerfiles/down-sampler/code
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      slot_timeout_seconds: 60
      # 1257010784/p00166/t1257010786_1257010845/ch117.fits
      key_group_regex: ^([0-9]+)/p([0-9]+)/t([0-9]+_[0-9]+)/ch([0-9]{3}).fits$
      key_group_index: 1,2,3
      bulk_message_size: 10
    environments:
      - KEEP_SOURCE_FILE=no
      - LOCAL_INPUT_ROOT=${LOCAL_SHM_ROOT}
      - LOCAL_OUTPUT_ROOT=${LOCAL_SHM_ROOT}
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      - ${NODES}:1

  # push 
  fits-redist:
    base_image: ${FILE_COPY}
    variables:
      task_timeout_seconds: 60
      dir_free_gb: ${FITS_REDIST_DIR_FREE_GB}
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      slot_timeout_seconds: 60
      # 1257010784/p00088/t1257010786_1257010845/ch119.fits.zst
      key_group_regex: ^([0-9]+)/p([0-9]+)/t([0-9]+_[0-9]+)/ch([0-9]{3}).fits.zst$
      key_group_index: 1,2,3
      bulk_message_size: 3
      # 255 12
      retry_rules: "['5:3','23:3']"
    environments:
      - TARGET_URL=/dev/shm/scalebox/mydata/mwa/1chx
      - SOURCE_MODE=${FITS_REDIST_MODE}
      - WITH_HEADERS=yes
      - KEEP_SOURCE_FILE=no
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      - ${NODES}:1

  fits-merger:
    label: 24通道fits合并
    base_image: ${MWA_VCSTOOLS}
    variables:
      task_timeout_seconds: ${FITS_MERGER_TIMEOUT}
      dir_free_gb: ${FITS_MERGER_DIR_FREE_GB}
      code_path: ${CODE_BASE}/dockerfiles/mwa-vcstools/fits-merger/code
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      # 1257010784/p00097/t1257010786_1257010845
      key_group_regex: ^([0-9]+)/p([0-9]{5})/t([0-9]+_[0-9]+)$
      key_group_index: 1,2,3
      bulk_message_size: 2
    environments:
      - KEEP_SOURCE_FILE=${KEEP_SOURCE_FILE}
      - LOCAL_INPUT_ROOT=${LOCAL_SHM_ROOT}
      - LOCAL_OUTPUT_ROOT=
      # - LOCAL_OUTPUT_ROOT=${LOCAL_SHM_ROOT}
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      - ${NODES}:1

  fits-24ch-push:
    base_image: ${FILE_COPY}
    variables:
      task_timeout_seconds: 1800
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      # 1257010784/p00016/t1257010786_1257010845.fits.zst
      key_group_regex: ^([0-9]+)/p([0-9]{5})/t([0-9]+_[0-9]+)$
      key_group_index: 1,3,2
      bulk_message_size: 2
      retry_rules: "['255:3']"
    environments:
      - KEEP_SOURCE_FILE=no
      # - SOURCE_URL=/dev/shm/scalebox/mydata/mwa/24ch
      - SOURCE_URL=${SHARED_ROOT}/24ch
      - TARGET_URL=${RESULT_24CH_URL}
      - TARGET_JUMP_SERVERS=${JUMP_SERVERS}
      - WITH_HEADERS=yes
    hosts:
      -  ${NODES}:1

  message-router-main:
    label: 主消息路由
    base_image: app-mwa/message-router-go
    parameters:
      key_group_regex:
      key_group_index:
      start_message: ${DATASET_URI}
      task_dist_mode: SLOT-BOUND
      bulk_message_size: 50
    environments:
      - LOG_LEVEL=warn
      - BATCH_INSERT=yes
      - DEFAULT_USER=${DEFAULT_USER}
      # 设置JUMP_SERVERS，则远端tar也通过local-tar-pull直接获取
      - JUMP_SERVERS=${JUMP_SERVERS}
      - DATASET_URI=${DATASET_URI}
      - NUM_OF_NODES=${NUM_OF_NODES}
      - LOCAL_IP_INDEX=${MESSAGE_ROUTER_LOCAL_IP_INDEX}
      - FITS_REDIST_MODE=${FITS_REDIST_MODE}
      # 依据计算节点列表，计算其名称前缀
      - NODES="${NODES}"
      - ENABLE_CLUSTER_DIST=yes
      - SHARED_ROOT=${SHARED_ROOT}
      # - TRACE=yes
    hosts:
      - h0:2
    sink_jobs:
      - dir-list
      - cluster-dist
      - pull-unpack
      - beam-maker
      - down-sampler
      - fits-redist
      - fits-merger
      - fits-24ch-push
