name: main.app-mwa
label: mwa-comput
cluster: ${CLUSTER}
parameters:
  initial_status: RUNNING
  message_router: message-router-main
  default_sleep_count: 600

jobs:
  dir-list:
    base_image: hub.cstcloud.cn/scalebox/dir-listx
    arguments:
      slot_options: slot_on_head
    environments:
      - REGEX_FILTER=${REGEX_FILTER}

  cluster-dist:
    base_image: ${FILE_COPY}
    arguments:
      code_path: ${CODE_BASE}/dockerfiles/cluster-dist/code
      task_timeout_seconds: 1800
      slot_options: tmpfs_workdir
      # 5TB读缓存的临时空间
      dir_limit_gb: /cluster_data_root/mwa/tar~3000
    parameters:
      # 1257010784/1257010786_1257010815_ch109.dat.tar.zst
      # 以时间排序的打包文件
      key_group_regex: ^([0-9]+)/([0-9]+_[0-9]+)_ch([0-9]+).+$
      key_group_index: 1,2,3
    environments:
      - TARGET_URL=/cluster_data_root/mwa/tar
    hosts:
      - ${IO_NODES}

  pull-unpack:
    base_image: ${FILE_COPY}
    # node1上的文件读取错误，需设置特权权限。可能更新该节点上docker到最新版本可解决该问题
    command: ${PRIVILEGED_COMMAND}
    arguments:
      code_path: ${CODE_BASE}/dockerfiles/pull-unpack/code
      # 避免工作目录放在 /tmp下，被定期删除
      slot_options: tmpfs_workdir
      task_timeout_seconds: 1800
      # 2 tar.zst files allowed in tmpfs
      # 下采样基本完成后，再重新传远端的打包文件（tar.zst）
      # SSD最多存放2组数据，beam-maker单次处理150秒，则最多存放300秒数据（300秒 * 313MB/秒 = 92GB ）
      dir_free_gb: ${UNPACK_DIR_FREE_GB}
      dir_limit_gb: ${UNPACK_DIR_LIMIT_GB}
      # 3 * 40s = 120s
      progress_counter_diff: 120
      # 每单位计算资源上的运行task数量
      # group_running_vtasks: 3
      # host_running_vtasks: 3
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      # 1301240224/1301244745_1301244784_ch120.dat.tar.zst~b00
      key_group_regex: ^([0-9]+)/([0-9]+)_[0-9]+_ch([0-9]+).+$
      key_group_index: 1,2,3
      # retry_rules: "['2:3']"
      pod_id: pod_0
    environments:
      - LOCAL_OUTPUT_ROOT=${LOCAL_DISK_ROOT}
      - JUMP_SERVERS=${JUMP_SERVERS}
      - BW_LIMIT=100m
      # 可给message-router发送重复名的消息（覆盖）
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
      - KEEP_SOURCE_FILE=yes
      # - KEEP_SOURCE_FILE=no
    hosts:
      -  ${NODES}:1

  beam-maker:
    label: beam-maker
    base_image: ${MWA_VCSTOOLS}
    command: ${ROCM_COMMAND}
    arguments:
      task_timeout_seconds: ${BEAM_MAKER_TIMEOUT}
      dir_free_gb: ${BEAM_MAKER_DIR_FREE_GB}
      code_path: ${CODE_BASE}/dockerfiles/mwa-vcstools/beam-maker/code
      # output_text_size: 1048576
      progress_counter_diff: 192
      slot_options: tmpfs_workdir
      sleep_interval_seconds: ${SLEEP_INTERVAL_SECONDS}
      max_sleep_count: ${NODE_MAX_SLEEP_COUNT}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      slot_timeout_seconds: 60
      # 1257010784/1257010786_1257010845/109/00001_00024
      key_group_regex: ^([0-9]+)/([0-9]+)_[0-9]+/([0-9]{3})/([0-9]{5})_[0-9]{5}$
      # 若节点少，每个节点处理多于一个channel，则为 1,3,2
      # 若节点多，每个节点处理1个channel，则顺序为：1,2,4
      key_group_index: 1,2,3
      # beam-maker的message-router耗时长，独立处理
      message_router_index: 1
      retry_rules: "['139:1','137:3','1:3']"
      pod_id: pod_0
    environments:
      - LOCAL_INPUT_ROOT=${LOCAL_DISK_ROOT}
      # - LOCAL_CAL_ROOT=${LOCAL_SHM_ROOT}
      - LOCAL_OUTPUT_ROOT=${LOCAL_SHM_ROOT}
      - ZSTD_TARGET_FILE=no
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
      # 通常无KEEP_SOURCE_FILE设置，由信号量统一管理何时删除原始文件，仅压力测试时需使用
    hosts:
      - ${NODES}:4

  down-sampler:
    base_image: ${DOWN_SAMPLER}
    command: ${PRIVILEGED_COMMAND}
    arguments:
      task_timeout_seconds: ${DOWN_SAMPLER_TIMEOUT}
      dir_free_gb: ${DOWN_SAMPLER_DIR_FREE_GB}
      code_path: ${CODE_BASE}/dockerfiles/down-sampler/code
      slot_options: tmpfs_workdir
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      slot_timeout_seconds: 60
      # 1257010784/p00166/t1257010786_1257010845/ch117.fits
      key_group_regex: (^[0-9]+)/p([0-9]+)/t([0-9]+)_[0-9]+/ch([0-9]{3}).fits$
      key_group_index: 1,3,2
      bulk_message_size: 10
      message_router_index: 2
      pod_id: pod_0
    environments:
      - KEEP_SOURCE_FILE=no
      - LOCAL_INPUT_ROOT=${LOCAL_SHM_ROOT}
      - LOCAL_OUTPUT_ROOT=${LOCAL_SHM_ROOT}
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      - ${NODES}:1

  # pull
  fits-redist:
    base_image: ${FILE_COPY}
    arguments:
      task_timeout_seconds: ${FITS_REDIST_TIMEOUT}
      dir_free_gb: ${FITS_REDIST_DIR_FREE_GB}
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      slot_timeout_seconds: 60
      # 1257010784/p00088/t1257010786_1257010845/ch119.fits.zst
      key_group_regex: ^([0-9]+)/p([0-9]+)/t([0-9]+)_[0-9]+/ch([0-9]{3}).fits.zst$
      key_group_index: 1,3,2
      bulk_message_size: 3
      # 255 12
      retry_rules: "['255:2','12:1']"
      message_router_index: 3
      slot_options: tmpfs_workdir
    environments:
      - TARGET_URL=/dev/shm/scalebox/mydata/mwa/1chx
      - SOURCE_MODE=${FITS_REDIST_MODE}
      - KEEP_SOURCE_FILE=no
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      - ${NODES}:1

  fits-merger:
    label: 24通道fits合并
    base_image: ${MWA_VCSTOOLS}
    arguments:
      task_timeout_seconds: ${FITS_MERGER_TIMEOUT}
      dir_free_gb: ${FITS_MERGER_DIR_FREE_GB}
      code_path: ${CODE_BASE}/dockerfiles/mwa-vcstools/fits-merger/code
      slot_options: tmpfs_workdir
    parameters:
      task_dist_mode: ${TASK_DIST_MODE}
      # 1257010784/p00097/t1257010786_1257010845
      key_group_regex: ^([0-9]+)/p([0-9]{5})/t([0-9]+)_[0-9]+$
      key_group_index: 1,3,2
      bulk_message_size: 2
    environments:
      - KEEP_SOURCE_FILE=${KEEP_SOURCE_FILE}
      - LOCAL_INPUT_ROOT=${LOCAL_SHM_ROOT}
      - LOCAL_OUTPUT_ROOT=/work1/cstu0036/mydata
      # - LOCAL_OUTPUT_ROOT=${LOCAL_SHM_ROOT}
      # "500k"/"1m"/...
      - BW_LIMIT=
      - SOFT_OUTPUT_MB=
      - HARD_OUTPUT_MB=
    hosts:
      - ${NODES}:1

  fits-24ch-push:
    base_image: ${FILE_COPY}
    arguments:
      task_timeout_seconds: 600
      slot_options: tmpfs_workdir
    parameters:
      task_dist_mode:
      # 1257010784/p00016/t1257010786_1257010845.fits.zst
      key_group_regex: ^([0-9]+)/p([0-9]{5})/t([0-9]+_[0-9]+)$
      key_group_index: 1,3,2
      bulk_message_size: 2
      retry_rules: "['255:3']"
    environments:
      - KEEP_SOURCE_FILE=no
      - SOURCE_URL=${CLUSTER_DATA_ROOT}/mwa/24ch
      - TARGET_URL=${RESULT_24CH_URL}
      - TARGET_JUMP_SERVERS=${JUMP_SERVERS}
    hosts:
      - ${IO_NODES}

  message-router-main:
    label: 主消息路由
    base_image: app-mwa/message-router-go
    parameters:
      # message format
      # from_job:dir-list, 1301240224/1301241065_1301241104_ch110.dat.tar.zst
      # from_job:cluster-dist, 1301240224/1301240345_1301240384_ch119.dat.tar.zst
      # from_job:pull-unpack, 1301240224_1301240226_ch113.dat~1301240224/1301240225_1301240264_ch113.dat.tar.zst
      # from_job:beam-maker, 1301240224/p05817/t1301240225_1301240424/ch117.fits
      # from_job:down-sampler, 1301240224/p05841/t1301240225_1301240424/ch117.fits.zst
      # from_job:fits-redist,  1301240224/p05843/t1301240225_1301240424/ch117.fits.zst
      # from_job:fits-merger, 1301240224/p05763/t1301240225_1301240424
      # from_job:fits-24ch-push, 1301240224/p05847/t1301240425_1301240624.fits.zst
      key_group_regex:
      key_group_index:
      start_message: ${DATASET_URI}
      task_dist_mode: SLOT-BOUND
      bulk_message_size: 100
    environments:
      - LOG_LEVEL=warn
      - BATCH_INSERT=yes
      - DEFAULT_USER=${DEFAULT_USER}
      # 设置JUMP_SERVERS，则远端tar也通过cluste-dist直接获取
      - JUMP_SERVERS=${JUMP_SERVERS}
      - DATASET_URI=${DATASET_URI}
      - NUM_OF_NODES=${NUM_OF_NODES}
      - LOCAL_IP_INDEX=${MESSAGE_ROUTER_LOCAL_IP_INDEX}
      - FITS_REDIST_MODE=${FITS_REDIST_MODE}
      # 依据计算节点列表，计算其名称前缀
      - NODES="${NODES}"
      - ENABLE_CLUSTER_DIST=yes
      # - TRACE=yes
    hosts:
      - h0:4
    sink_jobs:
      - dir-list
      - cluster-dist
      - pull-unpack
      - beam-maker
      - down-sampler
      - fits-redist
      - fits-merger
      - fits-24ch-push
